{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125611</th>\n",
       "      <td>040BD65CACCBE1A5</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2023-06-28 09:23:11</td>\n",
       "      <td>2023-06-28 09:27:40</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>Clinton St &amp; Washington Blvd</td>\n",
       "      <td>WL-012</td>\n",
       "      <td>41.889222</td>\n",
       "      <td>-87.638685</td>\n",
       "      <td>41.883380</td>\n",
       "      <td>-87.641170</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398181</th>\n",
       "      <td>D085175F9368C7E9</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2023-06-07 07:49:50</td>\n",
       "      <td>2023-06-07 07:56:24</td>\n",
       "      <td>State St &amp; Pearson St</td>\n",
       "      <td>TA1307000061</td>\n",
       "      <td>Stetson Ave &amp; South Water St</td>\n",
       "      <td>TA1308000029</td>\n",
       "      <td>41.897465</td>\n",
       "      <td>-87.628880</td>\n",
       "      <td>41.886835</td>\n",
       "      <td>-87.622320</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194909</th>\n",
       "      <td>C15806E66AB5E070</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2023-05-22 16:34:54</td>\n",
       "      <td>2023-05-22 16:43:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sedgwick St &amp; Huron St</td>\n",
       "      <td>TA1307000062</td>\n",
       "      <td>41.880000</td>\n",
       "      <td>-87.640000</td>\n",
       "      <td>41.894666</td>\n",
       "      <td>-87.638437</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328173</th>\n",
       "      <td>ECB907E0D930955C</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2023-04-15 10:43:04</td>\n",
       "      <td>2023-04-15 11:36:58</td>\n",
       "      <td>Calumet Ave &amp; 35th St</td>\n",
       "      <td>13345</td>\n",
       "      <td>Rhodes Ave &amp; 32nd St</td>\n",
       "      <td>13215</td>\n",
       "      <td>41.831390</td>\n",
       "      <td>-87.618012</td>\n",
       "      <td>41.836208</td>\n",
       "      <td>-87.613533</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96996</th>\n",
       "      <td>85C69FAE49B28361</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2022-12-16 08:12:37</td>\n",
       "      <td>2022-12-16 08:50:08</td>\n",
       "      <td>Sheffield Ave &amp; Webster Ave</td>\n",
       "      <td>TA1309000033</td>\n",
       "      <td>Dearborn St &amp; Adams St</td>\n",
       "      <td>TA1305000005</td>\n",
       "      <td>41.921540</td>\n",
       "      <td>-87.653818</td>\n",
       "      <td>41.879356</td>\n",
       "      <td>-87.629791</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ride_id  rideable_type           started_at  \\\n",
       "125611  040BD65CACCBE1A5  electric_bike  2023-06-28 09:23:11   \n",
       "398181  D085175F9368C7E9  electric_bike  2023-06-07 07:49:50   \n",
       "194909  C15806E66AB5E070  electric_bike  2023-05-22 16:34:54   \n",
       "328173  ECB907E0D930955C  electric_bike  2023-04-15 10:43:04   \n",
       "96996   85C69FAE49B28361   classic_bike  2022-12-16 08:12:37   \n",
       "\n",
       "                   ended_at           start_station_name start_station_id  \\\n",
       "125611  2023-06-28 09:27:40     Kingsbury St & Kinzie St     KA1503000043   \n",
       "398181  2023-06-07 07:56:24        State St & Pearson St     TA1307000061   \n",
       "194909  2023-05-22 16:43:14                          NaN              NaN   \n",
       "328173  2023-04-15 11:36:58        Calumet Ave & 35th St            13345   \n",
       "96996   2022-12-16 08:50:08  Sheffield Ave & Webster Ave     TA1309000033   \n",
       "\n",
       "                    end_station_name end_station_id  start_lat  start_lng  \\\n",
       "125611  Clinton St & Washington Blvd         WL-012  41.889222 -87.638685   \n",
       "398181  Stetson Ave & South Water St   TA1308000029  41.897465 -87.628880   \n",
       "194909        Sedgwick St & Huron St   TA1307000062  41.880000 -87.640000   \n",
       "328173          Rhodes Ave & 32nd St          13215  41.831390 -87.618012   \n",
       "96996         Dearborn St & Adams St   TA1305000005  41.921540 -87.653818   \n",
       "\n",
       "          end_lat    end_lng member_casual  \n",
       "125611  41.883380 -87.641170        member  \n",
       "398181  41.886835 -87.622320        member  \n",
       "194909  41.894666 -87.638437        member  \n",
       "328173  41.836208 -87.613533        casual  \n",
       "96996   41.879356 -87.629791        member  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV files and store them in variables\n",
    "data2208 = pd.read_csv(\"C:/Users/Administrator/Desktop/Data Science Project/cyclistic-bike-share/data/raw/202208-divvy-tripdata.csv\")\n",
    "data2209 = pd.read_csv(\"C:/Users/Administrator/Desktop/Data Science Project/cyclistic-bike-share/data/raw/202209-divvy-tripdata.csv\")\n",
    "data2210 = pd.read_csv(\"C:/Users/Administrator/Desktop/Data Science Project/cyclistic-bike-share/data/raw/202210-divvy-tripdata.csv\")\n",
    "data2211 = pd.read_csv(\"C:/Users/Administrator/Desktop/Data Science Project/cyclistic-bike-share/data/raw/202211-divvy-tripdata.csv\")\n",
    "data2212 = pd.read_csv(\"C:/Users/Administrator/Desktop/Data Science Project/cyclistic-bike-share/data/raw/202212-divvy-tripdata.csv\")\n",
    "data2301 = pd.read_csv(\"C:/Users/Administrator/Desktop/Data Science Project/cyclistic-bike-share/data/raw/202301-divvy-tripdata.csv\")\n",
    "data2302 = pd.read_csv(\"C:/Users/Administrator/Desktop/Data Science Project/cyclistic-bike-share/data/raw/202302-divvy-tripdata.csv\")\n",
    "data2303 = pd.read_csv(\"C:/Users/Administrator/Desktop/Data Science Project/cyclistic-bike-share/data/raw/202303-divvy-tripdata.csv\")\n",
    "data2304 = pd.read_csv(\"C:/Users/Administrator/Desktop/Data Science Project/cyclistic-bike-share/data/raw/202304-divvy-tripdata.csv\")\n",
    "data2305 = pd.read_csv(\"C:/Users/Administrator/Desktop/Data Science Project/cyclistic-bike-share/data/raw/202305-divvy-tripdata.csv\")\n",
    "data2306 = pd.read_csv(\"C:/Users/Administrator/Desktop/Data Science Project/cyclistic-bike-share/data/raw/202306-divvy-tripdata.csv\")\n",
    "data2307 = pd.read_csv(\"C:/Users/Administrator/Desktop/Data Science Project/cyclistic-bike-share/data/raw/202307-divvy-tripdata.csv\")\n",
    "\n",
    "# Concatenate all the dataframes into one\n",
    "df = pd.concat([data2208, data2209, data2210, data2211, data2212, data2301, data2302, data2303, data2304, data2305, data2306, data2307], ignore_index=False)\n",
    "\n",
    "# Sample 5 random rows from the dataframe\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a missing value that needs to be further investigated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5723606 entries, 0 to 767649\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   ride_id             object \n",
      " 1   rideable_type       object \n",
      " 2   started_at          object \n",
      " 3   ended_at            object \n",
      " 4   start_station_name  object \n",
      " 5   start_station_id    object \n",
      " 6   end_station_name    object \n",
      " 7   end_station_id      object \n",
      " 8   start_lat           float64\n",
      " 9   start_lng           float64\n",
      " 10  end_lat             float64\n",
      " 11  end_lng             float64\n",
      " 12  member_casual       object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 611.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display information about the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"started_at\" and \"ended_at\" columns should be of the datetime data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ride_id               5723606\n",
       "rideable_type               3\n",
       "started_at            4821003\n",
       "ended_at              4833101\n",
       "start_station_name       1801\n",
       "start_station_id         1502\n",
       "end_station_name         1799\n",
       "end_station_id           1505\n",
       "start_lat              782360\n",
       "start_lng              740459\n",
       "end_lat                 13868\n",
       "end_lng                 13983\n",
       "member_casual               2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of unique values in each column of the DataFrame\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The start_station_id represents the start_station_name, and vice versa, so the number of unique values should be the same for both. The same applies to end_station_id and end_station_name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride_id                    0\n",
      "rideable_type              0\n",
      "started_at                 0\n",
      "ended_at                   0\n",
      "start_station_name    868772\n",
      "start_station_id      868904\n",
      "end_station_name      925008\n",
      "end_station_id        925149\n",
      "start_lat                  0\n",
      "start_lng                  0\n",
      "end_lat                 6102\n",
      "end_lng                 6102\n",
      "member_casual              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each column of the dataframe\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Print the count of missing values for each column\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride_id                0.000000\n",
      "rideable_type          0.000000\n",
      "started_at             0.000000\n",
      "ended_at               0.000000\n",
      "start_station_name    15.178753\n",
      "start_station_id      15.181059\n",
      "end_station_name      16.161280\n",
      "end_station_id        16.163744\n",
      "start_lat              0.000000\n",
      "start_lng              0.000000\n",
      "end_lat                0.106611\n",
      "end_lng                0.106611\n",
      "member_casual          0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of missing values in each column\n",
    "missing_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Print the percentage of missing values for each column\n",
    "print(missing_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider removing rows that contain null values, especially when these null values are not widely distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride_id               0\n",
      "rideable_type         0\n",
      "started_at            0\n",
      "ended_at              0\n",
      "start_station_name    0\n",
      "start_station_id      0\n",
      "end_station_name      0\n",
      "end_station_id        0\n",
      "start_lat             0\n",
      "start_lng             0\n",
      "end_lat               0\n",
      "end_lng               0\n",
      "member_casual         0\n",
      "dtype: int64 (4340367, 13)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values from the DataFrame\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Check for any remaining missing values in the cleaned DataFrame\n",
    "print(df_cleaned.isnull().sum(),df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Duplicated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of duplicated rows in the dataframe\n",
    "df_cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_20988\\2860144393.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['started_at'] = pd.to_datetime(df_cleaned['started_at'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started_at    datetime64[ns]\n",
      "ended_at      datetime64[ns]\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_20988\\2860144393.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['ended_at'] = pd.to_datetime(df_cleaned['ended_at'])\n"
     ]
    }
   ],
   "source": [
    "# Convert 'started_at' column to datetime\n",
    "df_cleaned['started_at'] = pd.to_datetime(df_cleaned['started_at'])\n",
    "\n",
    "# Convert 'ended_at' column to datetime\n",
    "df_cleaned['ended_at'] = pd.to_datetime(df_cleaned['ended_at'])\n",
    "\n",
    "# Verify the changes\n",
    "print(df_cleaned[['started_at', 'ended_at']].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Consistency Data and Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ride_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ride_id\n",
       "16    4340367\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of unique lengths of ride_id in df_cleaned\n",
    "df_cleaned['ride_id'].str.len().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rideable_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rideable_type\n",
       "classic_bike     2480774\n",
       "electric_bike    1733504\n",
       "docked_bike       126089\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of occurrences of each rideable type in the cleaned dataframe\n",
    "df_cleaned['rideable_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### started_at > ended_at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'ended_at' value must be greater than the 'started_at' value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where \"ended_at\" is less than \"started_at\": 370\n"
     ]
    }
   ],
   "source": [
    "# Check for rows where 'ended_at' is less than 'started_at'\n",
    "ended_at_less_than_started = df_cleaned[df_cleaned['ended_at'] <= df_cleaned['started_at']]\n",
    "\n",
    "# display the number of rows where 'ended_at' is less than 'started_at'\n",
    "print('Number of rows where \"ended_at\" is less than \"started_at\":', len(ended_at_less_than_started))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4340302, 13)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where 'ended_at' is less than 'started_at'\n",
    "df_cleaned = df_cleaned[df_cleaned['ended_at'] >= df_cleaned['started_at']]\n",
    "\n",
    "# display shape of updated dataframe\n",
    "print(df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start_station_id v start_station_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of start_station_id with more than one start_station_name:\n",
      " ['1032', '1033', '1038', '1039', '13074', '13197', '13259', '15541', '15623', '20133', '303', '335.0', '390', '410', '514', '515', '517', '518', '519', '520', '523', '528', '534', '535', '536', '537', '540', '543', '545', '546', '549', '553', '554', '556', '559', '560', '561', '562', '564', '567', '569', '571', '572', '573', '574', '575', '577', '579', '580', '584', '585', '586', '588', '592', '594', '595', '596', '605', '612', '613', '614', '617', '620', '621', '623', '624', '627', '629', '631', '636', '637', '638', '639', '641', '642', '644', '646', '649', '650', '651', '652', '653', '657', '658', '661', '662', '663', '667', '672', '677', '678', '679', '681', '682', '683', '687', '690', '691', '692', '693', '695', '698', '699', '702', '703', '706', '710', '711', '714', '715', '718', '719', '720', '722', '724', '730', '737', '738', '739', '744', '745', '749', '750', '751', '753', '755', '756', '757', '758', '759', '760', '762', '763', '766', '767', '768', '772', '774', '775', '776', '777', '783', '784', '785', '786', '788', '793', '794', '795', '799', '800', '801', '803', '805', '809', '810', '812', '813', '814', '815', '819', '821', '822', '823', '824', '825', '826', '828', '829', '830', '831', '832', '833', '834', '835', '836', '839', '840', '841', '844', '847', '852', '855', '866', '871', '875', '877', '883', '887', '890', '898', '899', '900', '902', '903', '904', '911', '913', '914', '915', '916', '918', '919', '920', '950', '951', '954', 'DIVVY 001 - Warehouse test station', 'Hubbard Bike-checking (LBS-WH-TEST)', 'KA1503000055', 'KA1504000086', 'KA1504000141', 'KA1504000168', 'TA1306000026', 'TA1308000026', 'TA1309000042', 'chargingstx06', 'chargingstx07']\n",
      "Number of the result: 218\n"
     ]
    }
   ],
   "source": [
    "# Group the dataframe by 'start_station_id' and count the unique 'start_station_name' for each group\n",
    "start_station_id_group = df_cleaned.groupby('start_station_id')['start_station_name'].nunique().reset_index()\n",
    "\n",
    "# Filter out the groups where 'start_station_id' has more than one unique 'start_station_name'\n",
    "multiple_start_station_ids = start_station_id_group[start_station_id_group['start_station_name'] > 1]['start_station_id'].tolist()\n",
    "\n",
    "# Print the list of 'start_station_id' with more than one unique 'start_station_name'\n",
    "print('List of start_station_id with more than one start_station_name:\\n', multiple_start_station_ids)\n",
    "print('Number of the result:', len(multiple_start_station_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'start_station_id' and find the most frequent 'start_station_name'\n",
    "most_frequent_names = df.groupby('start_station_id')['start_station_name'].agg(lambda x: x.mode().iloc[0])\n",
    "\n",
    "# replace the 'start_station_id' with the most frequent 'start_station_name' in each start_station_id that has more than one unique 'start_station_name'\n",
    "df_cleaned['start_station_name'] = df_cleaned['start_station_id'].map(most_frequent_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of start_station_id with more than one start_station_name:\n",
      " []\n",
      "Number of the result: 0\n"
     ]
    }
   ],
   "source": [
    "# Recheck by group the dataframe by 'start_station_id' and count the unique 'start_station_name' for each group\n",
    "start_station_id_group2 = df_cleaned.groupby('start_station_id')['start_station_name'].nunique().reset_index()\n",
    "\n",
    "# Filter out the groups where 'start_station_id' has more than one unique 'start_station_name'\n",
    "multiple_start_station_ids2 = start_station_id_group2[start_station_id_group2['start_station_name'] > 1]['start_station_id'].tolist()\n",
    "\n",
    "# Print the list of 'start_station_id' with more than one unique 'start_station_name'\n",
    "print('List of start_station_id with more than one start_station_name:\\n', multiple_start_station_ids2)\n",
    "print('Number of the result:', len(multiple_start_station_ids2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start_station_name v start_station_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of start_station_name with more than one start_station_id:\n",
      " ['Jeffery Blvd & 67th St', 'Lake Park Ave & 47th St', 'Wilton Ave & Diversey Pkwy*']\n",
      "Number of the result: 3\n"
     ]
    }
   ],
   "source": [
    "# Group the dataframe by 'start_station_name' and count the unique 'start_station_id' for each group\n",
    "start_station_name_group = df_cleaned.groupby('start_station_name')['start_station_id'].nunique().reset_index()\n",
    "\n",
    "# Filter out the groups where 'start_station_name' has more than one unique 'start_station_id'\n",
    "multiple_start_station_names = start_station_name_group[start_station_name_group['start_station_id'] > 1]['start_station_name'].tolist()\n",
    "\n",
    "# Print the list of 'start_station_name' with more than one unique 'start_station_id'\n",
    "print('List of start_station_name with more than one start_station_id:\\n', multiple_start_station_names)\n",
    "print('Number of the result:', len(multiple_start_station_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'start_station_name' and find the most frequent 'start_station_id'\n",
    "most_frequent_ids = df.groupby('start_station_name')['start_station_id'].agg(lambda x: x.mode().iloc[0])\n",
    "\n",
    "# replace the 'start_station_name' with the most frequent 'start_station_id' in each start_station_name that has more than one unique 'start_station_id'\n",
    "df_cleaned['start_station_id'] = df_cleaned['start_station_name'].map(most_frequent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of start_station_name with more than one start_station_id:\n",
      " []\n",
      "Number of the result: 0\n"
     ]
    }
   ],
   "source": [
    "# Recheck by group the dataframe by 'start_station_name' and count the unique 'start_station_id' for each group\n",
    "start_station_name_group2 = df_cleaned.groupby('start_station_name')['start_station_id'].nunique().reset_index()\n",
    "\n",
    "# Filter out the groups where 'start_station_name' has more than one unique 'start_station_id'\n",
    "multiple_start_station_names2 = start_station_name_group2[start_station_name_group2['start_station_id'] > 1]['start_station_name'].tolist()\n",
    "\n",
    "# Print the list of 'start_station_name' with more than one unique 'start_station_id'\n",
    "print('List of start_station_name with more than one start_station_id:\\n', multiple_start_station_names2)\n",
    "print('Number of the result:', len(multiple_start_station_names2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end_station_id v end_station_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of end_station_id with more than one end_station_name:\n",
      " ['1032', '1039', '13074', '13197', '13259', '15541', '15623', '20133', '303', '335.0', '390', '410', '514', '515', '517', '518', '519', '520', '523', '525', '528', '534', '535', '536', '537', '540', '543', '545', '546', '549', '553', '554', '557', '559', '560', '561', '562', '564', '567', '569', '571', '572', '573', '574', '575', '576', '577', '579', '580', '583', '584', '585', '586', '587', '588', '592', '594', '595', '596', '604', '605', '610', '611', '614', '617', '620', '621', '623', '624', '626', '627', '628', '631', '636', '637', '638', '639', '640', '641', '642', '644', '646', '647', '649', '650', '651', '652', '653', '655', '657', '658', '661', '662', '663', '665', '672', '674', '677', '679', '681', '682', '683', '686', '687', '689', '690', '691', '692', '693', '694', '695', '696', '697', '698', '699', '702', '706', '710', '714', '715', '718', '720', '722', '724', '730', '731', '733', '735', '736', '737', '738', '739', '742', '744', '745', '749', '750', '751', '756', '757', '758', '759', '760', '762', '763', '766', '767', '768', '772', '774', '775', '776', '777', '779', '783', '786', '788', '789', '793', '795', '800', '801', '805', '808', '809', '810', '811', '812', '813', '814', '815', '819', '821', '822', '823', '824', '825', '826', '827', '828', '829', '830', '831', '832', '833', '834', '835', '836', '839', '841', '844', '851', '855', '866', '868', '871', '873', '876', '877', '878', '883', '885', '899', '902', '904', '906', '911', '913', '914', '915', '916', '918', '919', '920', '922', '923', '927', '950', '951', '954', 'KA1503000055', 'KA1504000086', 'KA1504000141', 'KA1504000168', 'TA1306000026', 'TA1308000026', 'TA1309000042', 'chargingstx06', 'chargingstx07']\n",
      "Number of the result: 229\n"
     ]
    }
   ],
   "source": [
    "# Group the dataframe by 'end_station_id' and count the unique 'end_station_name' for each group\n",
    "end_station_id_group = df_cleaned.groupby('end_station_id')['end_station_name'].nunique().reset_index()\n",
    "\n",
    "# Filter out the groups where 'end_station_id' has more than one unique 'end_station_name'\n",
    "multiple_end_station_ids = end_station_id_group[end_station_id_group['end_station_name'] > 1]['end_station_id'].tolist()\n",
    "\n",
    "# Print the list of 'end_station_id' with more than one unique 'end_station_name'\n",
    "print('List of end_station_id with more than one end_station_name:\\n', multiple_end_station_ids)\n",
    "print('Number of the result:', len(multiple_end_station_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'end_station_id' and find the most frequent 'end_station_name'\n",
    "most_frequent_names = df.groupby('end_station_id')['end_station_name'].agg(lambda x: x.mode().iloc[0])\n",
    "\n",
    "# replace the 'end_station_id' with the most frequent 'end_station_name' in each end_station_id that has more than one unique 'end_station_name'\n",
    "df_cleaned['end_station_name'] = df_cleaned['end_station_id'].map(most_frequent_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of end_station_id with more than one end_station_name:\n",
      " []\n",
      "Number of the result: 0\n"
     ]
    }
   ],
   "source": [
    "# Group the dataframe by 'end_station_id' and count the unique 'end_station_name' for each group\n",
    "end_station_id_group = df_cleaned.groupby('end_station_id')['end_station_name'].nunique().reset_index()\n",
    "\n",
    "# Filter out the groups where 'end_station_id' has more than one unique 'end_station_name'\n",
    "multiple_end_station_ids = end_station_id_group[end_station_id_group['end_station_name'] > 1]['end_station_id'].tolist()\n",
    "\n",
    "# Print the list of 'end_station_id' with more than one unique 'end_station_name'\n",
    "print('List of end_station_id with more than one end_station_name:\\n', multiple_end_station_ids)\n",
    "print('Number of the result:', len(multiple_end_station_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end_station_name v end_station_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of end_station_name with more than one end_station_id:\n",
      " ['East End Ave & 87th St', 'Jeffery Blvd & 67th St', 'Lake Park Ave & 47th St', 'Lawndale Ave & 111th St', 'Wilton Ave & Diversey Pkwy*']\n",
      "Number of the result: 5\n"
     ]
    }
   ],
   "source": [
    "# Group the dataframe by 'end_station_name' and count the unique 'end_station_id' for each group\n",
    "end_station_name_group = df_cleaned.groupby('end_station_name')['end_station_id'].nunique().reset_index()\n",
    "\n",
    "# Filter out the groups where 'end_station_name' has more than one unique 'end_station_id'\n",
    "multiple_end_station_names = end_station_name_group[end_station_name_group['end_station_id'] > 1]['end_station_name'].tolist()\n",
    "\n",
    "# Print the list of 'end_station_name' with more than one unique 'end_station_id'\n",
    "print('List of end_station_name with more than one end_station_id:\\n', multiple_end_station_names)\n",
    "print('Number of the result:', len(multiple_end_station_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'end_station_name' and find the most frequent 'end_station_id'\n",
    "most_frequent_ids = df.groupby('end_station_name')['end_station_id'].agg(lambda x: x.mode().iloc[0])\n",
    "\n",
    "# Replace the 'end_station_name' with the most frequent 'end_station_id' in each end_station_name that has more than one unique 'end_station_id'\n",
    "df_cleaned['end_station_id'] = df_cleaned['end_station_name'].map(most_frequent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of end_station_name with more than one end_station_id:\n",
      " []\n",
      "Number of the result: 0\n"
     ]
    }
   ],
   "source": [
    "# Group the dataframe by 'end_station_name' and count the unique 'end_station_id' for each group\n",
    "end_station_name_group = df_cleaned.groupby('end_station_name')['end_station_id'].nunique().reset_index()\n",
    "\n",
    "# Filter out the groups where 'end_station_name' has more than one unique 'end_station_id'\n",
    "multiple_end_station_names = end_station_name_group[end_station_name_group['end_station_id'] > 1]['end_station_name'].tolist()\n",
    "\n",
    "# Print the list of 'end_station_name' with more than one unique 'end_station_id'\n",
    "print('List of end_station_name with more than one end_station_id:\\n', multiple_end_station_names)\n",
    "print('Number of the result:', len(multiple_end_station_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude values are consistent.\n"
     ]
    }
   ],
   "source": [
    "# Check if the latitude values are within the valid range\n",
    "if df_cleaned['start_lat'].min() >= -90 and df_cleaned['start_lat'].max() <= 90:\n",
    "    print(\"Latitude values are consistent.\")\n",
    "else:\n",
    "    print(\"Latitude values are inconsistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start_lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitude values are consistent.\n"
     ]
    }
   ],
   "source": [
    "# Check if the longitude values are within the valid range\n",
    "if df_cleaned['start_lng'].min() >= -180 and df_cleaned['start_lng'].max() <= 180:\n",
    "    print(\"Longitude values are consistent.\")\n",
    "else:\n",
    "    print(\"Longitude values are inconsistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude values are consistent.\n"
     ]
    }
   ],
   "source": [
    "# Check if the latitude values are within the valid range\n",
    "if df['end_lat'].min() >= -90 and df['end_lat'].max() <= 90:\n",
    "    print(\"Latitude values are consistent.\")\n",
    "else:\n",
    "    print(\"Latitude values are inconsistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end_lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitude values are consistent.\n"
     ]
    }
   ],
   "source": [
    "# Check if the longitude values are within the valid range\n",
    "if df_cleaned['end_lng'].min() >= -180 and df_cleaned['end_lng'].max() <= 180:\n",
    "    print(\"Longitude values are consistent.\")\n",
    "else:\n",
    "    print(\"Longitude values are inconsistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>ride_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>2023-07-03 09:11:17</td>\n",
       "      <td>2023-07-03 09:16:01</td>\n",
       "      <td>284.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89339</th>\n",
       "      <td>2023-06-29 13:38:03</td>\n",
       "      <td>2023-06-29 13:54:27</td>\n",
       "      <td>984.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559820</th>\n",
       "      <td>2022-08-13 12:36:54</td>\n",
       "      <td>2022-08-13 13:10:38</td>\n",
       "      <td>2024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154739</th>\n",
       "      <td>2022-10-07 20:55:26</td>\n",
       "      <td>2022-10-07 21:19:33</td>\n",
       "      <td>1447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242688</th>\n",
       "      <td>2023-04-19 21:43:52</td>\n",
       "      <td>2023-04-19 22:01:20</td>\n",
       "      <td>1048.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                started_at            ended_at  ride_length\n",
       "3084   2023-07-03 09:11:17 2023-07-03 09:16:01        284.0\n",
       "89339  2023-06-29 13:38:03 2023-06-29 13:54:27        984.0\n",
       "559820 2022-08-13 12:36:54 2022-08-13 13:10:38       2024.0\n",
       "154739 2022-10-07 20:55:26 2022-10-07 21:19:33       1447.0\n",
       "242688 2023-04-19 21:43:52 2023-04-19 22:01:20       1048.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create ride_length column in seconds by subtracting the 'ended_at' from the 'started_at'\n",
    "df_cleaned['ride_length'] = (df_cleaned['ended_at'] - df_cleaned['started_at']).dt.total_seconds()\n",
    "\n",
    "# display randomly selected rows\n",
    "df_cleaned[['started_at', 'ended_at', 'ride_length']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>started_at</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53002</th>\n",
       "      <td>2023-02-01 19:39:31</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428623</th>\n",
       "      <td>2023-07-30 14:15:51</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413132</th>\n",
       "      <td>2022-08-24 15:41:06</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368982</th>\n",
       "      <td>2023-07-08 17:14:38</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152306</th>\n",
       "      <td>2022-11-02 08:04:16</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                started_at day_of_week\n",
       "53002  2023-02-01 19:39:31   Wednesday\n",
       "428623 2023-07-30 14:15:51      Sunday\n",
       "413132 2022-08-24 15:41:06   Wednesday\n",
       "368982 2023-07-08 17:14:38    Saturday\n",
       "152306 2022-11-02 08:04:16   Wednesday"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create day_of_week column from the 'started_at' column\n",
    "df_cleaned['day_of_week'] = df_cleaned['started_at'].dt.day_name()\n",
    "\n",
    "# display randomly selected rows\n",
    "df_cleaned[['started_at', 'day_of_week']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>started_at</th>\n",
       "      <th>start_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191480</th>\n",
       "      <td>2022-09-24 15:39:00</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406375</th>\n",
       "      <td>2023-06-22 08:43:01</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130116</th>\n",
       "      <td>2023-04-27 08:07:14</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29633</th>\n",
       "      <td>2023-04-06 12:23:18</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496617</th>\n",
       "      <td>2022-10-27 18:42:27</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                started_at  start_time\n",
       "191480 2022-09-24 15:39:00          15\n",
       "406375 2023-06-22 08:43:01           8\n",
       "130116 2023-04-27 08:07:14           8\n",
       "29633  2023-04-06 12:23:18          12\n",
       "496617 2022-10-27 18:42:27          18"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create start_time column by categorizing the 'started_at' column by hour\n",
    "df_cleaned['start_time'] = df_cleaned['started_at'].dt.hour\n",
    "\n",
    "# display randomly selected rows\n",
    "df_cleaned[['started_at', 'start_time']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>trip_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>268611</th>\n",
       "      <td>41.899930</td>\n",
       "      <td>-87.634430</td>\n",
       "      <td>41.891023</td>\n",
       "      <td>-87.635480</td>\n",
       "      <td>0.993107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258610</th>\n",
       "      <td>41.867906</td>\n",
       "      <td>-87.623044</td>\n",
       "      <td>41.867888</td>\n",
       "      <td>-87.623041</td>\n",
       "      <td>0.002050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164127</th>\n",
       "      <td>41.878061</td>\n",
       "      <td>-87.644230</td>\n",
       "      <td>41.871840</td>\n",
       "      <td>-87.646640</td>\n",
       "      <td>0.719387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108459</th>\n",
       "      <td>41.938429</td>\n",
       "      <td>-87.698008</td>\n",
       "      <td>41.918306</td>\n",
       "      <td>-87.636282</td>\n",
       "      <td>5.586366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268703</th>\n",
       "      <td>41.791478</td>\n",
       "      <td>-87.599861</td>\n",
       "      <td>41.799568</td>\n",
       "      <td>-87.594747</td>\n",
       "      <td>0.994014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        start_lat  start_lng    end_lat    end_lng  trip_distance\n",
       "268611  41.899930 -87.634430  41.891023 -87.635480       0.993107\n",
       "258610  41.867906 -87.623044  41.867888 -87.623041       0.002050\n",
       "164127  41.878061 -87.644230  41.871840 -87.646640       0.719387\n",
       "108459  41.938429 -87.698008  41.918306 -87.636282       5.586366\n",
       "268703  41.791478 -87.599861  41.799568 -87.594747       0.994014"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming your dataframe is named df_cleaned\n",
    "df_cleaned['trip_distance'] = df_cleaned.apply(lambda row: geodesic((row['start_lat'], row['start_lng']), (row['end_lat'], row['end_lng'])).kilometers, axis=1)\n",
    "\n",
    "# display randomly selected rows\n",
    "df_cleaned[['start_lat', 'start_lng', 'end_lat', 'end_lng', 'trip_distance']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataframe to csv on specified path\n",
    "# df_cleaned.to_csv('C:/Users/Administrator/Desktop/Data Science Project/cyclistic-bike-share/data/processed/df_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
